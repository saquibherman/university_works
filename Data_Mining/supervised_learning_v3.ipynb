{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29620b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-smarthome-m.europe-north1-a.c.smarthome-326501.internal:44925\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SupervisedLearning</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7faba3d65eb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType, IntegerType, StringType, FloatType\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "                      \n",
    "spark = SparkSession.builder.appName(\"SupervisedLearning\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aa9eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor_id: integer (nullable = true)\n",
      " |-- value_id: integer (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- node_id: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sensor = spark.read.parquet('gs://smarthome-326501/sensor_full.parquet')\n",
    "sensor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2301b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_counts = sensor.rdd.getNumPartitions()\n",
    "partition_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af92507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|                 key|  value|\n",
      "+--------------------+-------+\n",
      "|spark.sql.files.m...|6000000|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"set spark.sql.files.maxPartitionBytes=1000000\").show()\n",
    "spark.sql(\"set spark.sql.files.maxPartitionBytes=6000000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dd3e9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sensor = spark.read.parquet('gs://smarthome-326501/sensor_full.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d16249ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor = sensor.repartition(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2589b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e7c7b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.sql.adaptiv...| 1000|\n",
      "+--------------------+-----+\n",
      "\n",
      "+--------------------+---------+\n",
      "|                 key|    value|\n",
      "+--------------------+---------+\n",
      "|spark.sql.adaptiv...|209715200|\n",
      "+--------------------+---------+\n",
      "\n",
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.default.par...| 1000|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.adaptive.coalescePartitions.initialPartitionNum=1000\").show()\n",
    "spark.sql(\"set spark.sql.adaptive.advisoryPartitionSizeInBytes=209715200\").show()\n",
    "spark.sql(\"set spark.default.parallelism=1000\").show()\n",
    "# spark.sql(\"set spark.sql.shuffle.partitions=1000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65044c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "686"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_counts = sensor.rdd.getNumPartitions()\n",
    "partition_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f10ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "smarthome_df = sensor.withColumn(\"timestamp\", date_format(sensor.timestamp,\"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b8c0e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pivotDF = smarthome_df.groupBy(\"timestamp\").pivot(\"name\").sum(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14cc97b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotDF = pivotDF.withColumn(\"timestamp\", to_timestamp(pivotDF.timestamp)).sort(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8154023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotDF = pivotDF.withColumn('ts_day', date_format('timestamp', 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf0ecc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import last\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('ts_day')\\\n",
    "               .orderBy('timestamp')\\\n",
    "               .rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "\n",
    "# do the fill\n",
    "pivotDF = pivotDF.withColumn('entrance/door/contact', last(pivotDF['entrance/door/contact'], ignorenulls=True).over(window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dad3a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "# define the window\n",
    "window = Window.partitionBy('ts_day')\\\n",
    "               .orderBy('timestamp')\\\n",
    "               .rowsBetween(0,sys.maxsize)\n",
    "\n",
    "# do the fill\n",
    "pivotDF = pivotDF.withColumn('entrance/door/contact', first(pivotDF['entrance/door/contact'], ignorenulls=True).over(window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b1740ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotDF = pivotDF.fillna({'balcon/door/contact':0,\n",
    "                          'bathroom/ambience/motion':0,\n",
    "                          'bedroom/ambience/motion':0,\n",
    "                          'bedroom/ambience_under_the_bed/motion':0,\n",
    "                          'corridor/ambience/motion':0,\n",
    "#                           'entrance/door/contact':0,\n",
    "                          'kitchen/ambience/motion':0,\n",
    "                          'kitchen/fridge/contact':0,\n",
    "                          'livingroom/ambience/motion':0,\n",
    "                          \"kitchen/coffeemaker/current\":0,\n",
    "                          \"kitchen/sandwichmaker/current\":0, \n",
    "                          \"kitchen/dishwasher/current\":0,\n",
    "                          \"kitchen/kettle/current\":0, \n",
    "                          \"bathroom/washingmachine/current\":0, \n",
    "                          \"kitchen/microwave/current\":0,\n",
    "                          \"corridor/ilifeRobot/current\":0\n",
    "                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea4dbb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotDF = pivotDF.fillna({\"bathroom/ambience/light\": 0,\n",
    "           \"bedroom/bed/pressure\" : 480,\n",
    "           \"livingroom/tv/light\": 3,\n",
    "           \"bedroom/weightscale/pressure\" : 0,\n",
    "           \"bathroom/ambience/humidity\" : 15.2,\n",
    "           \"bathroom/ambience/temperature\": 21.13,\n",
    "           \"kitchen/stove/light\": 0,\n",
    "           \"livingroom/couch/pressure\": 0\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ff2068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotDF = pivotDF.withColumn('kitchen/coffeemaker/current',(when(pivotDF[\"kitchen/coffeemaker/current\"] > 1, 1).otherwise(0)))\n",
    "pivotDF = pivotDF.withColumn('kitchen/sandwichmaker/current',(when(pivotDF[\"kitchen/sandwichmaker/current\"] > 1, 1).otherwise(0)))\n",
    "pivotDF = pivotDF.withColumn('kitchen/dishwasher/current',(when(pivotDF[\"kitchen/dishwasher/current\"] > 1, 1).otherwise(0)))\n",
    "pivotDF = pivotDF.withColumn('kitchen/kettle/current',(when(pivotDF[\"kitchen/kettle/current\"] > 1, 1).otherwise(0)))\n",
    "pivotDF = pivotDF.withColumn('bathroom/washingmachine/current',(when(pivotDF[\"bathroom/washingmachine/current\"] > 1, 1).otherwise(0)))\n",
    "pivotDF = pivotDF.withColumn('kitchen/microwave/current',(when(pivotDF[\"kitchen/microwave/current\"] > 1, 1).otherwise(0)))\n",
    "pivotDF = pivotDF.withColumn('corridor/ilifeRobot/current',(when(pivotDF[\"corridor/ilifeRobot/current\"] > 1, 1).otherwise(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc4634e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivotDF.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69b20287",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_value = 0\n",
    "counter = 0\n",
    "status = 1\n",
    "\n",
    "def getStatus(new_value):\n",
    "    global old_value\n",
    "    global counter\n",
    "    global status\n",
    "\n",
    "    if counter != 0:\n",
    "        counter = counter - 1\n",
    "        old_value = new_value\n",
    "        return status\n",
    "\n",
    "    if old_value != new_value:\n",
    "        status = 1 - status\n",
    "        counter = 30\n",
    "\n",
    "    old_value = new_value\n",
    "\n",
    "    return status\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfOccupancy = udf(getStatus)\n",
    "\n",
    "# Create a new column using your UDF\n",
    "pivotDF = pivotDF.withColumn('occupied', udfOccupancy(pivotDF[\"entrance/door/contact\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93396301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/19 09:24:27 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>balcon/door/contact</th>\n",
       "      <th>bathroom/ambience/humidity</th>\n",
       "      <th>bathroom/ambience/light</th>\n",
       "      <th>bathroom/ambience/motion</th>\n",
       "      <th>bathroom/ambience/temperature</th>\n",
       "      <th>bathroom/washingmachine/current</th>\n",
       "      <th>bedroom/ambience/motion</th>\n",
       "      <th>bedroom/ambience_under_the_bed/motion</th>\n",
       "      <th>bedroom/bed/pressure</th>\n",
       "      <th>...</th>\n",
       "      <th>kitchen/fridge/contact</th>\n",
       "      <th>kitchen/kettle/current</th>\n",
       "      <th>kitchen/microwave/current</th>\n",
       "      <th>kitchen/sandwichmaker/current</th>\n",
       "      <th>kitchen/stove/light</th>\n",
       "      <th>livingroom/ambience/motion</th>\n",
       "      <th>livingroom/couch/pressure</th>\n",
       "      <th>livingroom/tv/light</th>\n",
       "      <th>ts_day</th>\n",
       "      <th>occupied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-01 00:00:01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-01 00:00:02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-01 00:00:03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-01 00:00:04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  balcon/door/contact  bathroom/ambience/humidity  \\\n",
       "0 2020-03-01 00:00:00                  0.0                        15.2   \n",
       "1 2020-03-01 00:00:01                  0.0                        15.2   \n",
       "2 2020-03-01 00:00:02                  0.0                        15.2   \n",
       "3 2020-03-01 00:00:03                  0.0                        15.2   \n",
       "4 2020-03-01 00:00:04                  0.0                        15.2   \n",
       "\n",
       "   bathroom/ambience/light  bathroom/ambience/motion  \\\n",
       "0                      0.0                       0.0   \n",
       "1                      0.0                       0.0   \n",
       "2                      0.0                       0.0   \n",
       "3                      0.0                       0.0   \n",
       "4                      0.0                       0.0   \n",
       "\n",
       "   bathroom/ambience/temperature  bathroom/washingmachine/current  \\\n",
       "0                          21.13                                0   \n",
       "1                          21.13                                0   \n",
       "2                          21.13                                0   \n",
       "3                          21.13                                0   \n",
       "4                          21.13                                0   \n",
       "\n",
       "   bedroom/ambience/motion  bedroom/ambience_under_the_bed/motion  \\\n",
       "0                      0.0                                    0.0   \n",
       "1                      0.0                                    0.0   \n",
       "2                      0.0                                    0.0   \n",
       "3                      0.0                                    0.0   \n",
       "4                      0.0                                    0.0   \n",
       "\n",
       "   bedroom/bed/pressure  ...  kitchen/fridge/contact  kitchen/kettle/current  \\\n",
       "0                 619.0  ...                     0.0                       0   \n",
       "1                 480.0  ...                     0.0                       0   \n",
       "2                 619.0  ...                     0.0                       0   \n",
       "3                 619.0  ...                     0.0                       0   \n",
       "4                 619.0  ...                     0.0                       0   \n",
       "\n",
       "   kitchen/microwave/current  kitchen/sandwichmaker/current  \\\n",
       "0                          0                              0   \n",
       "1                          0                              0   \n",
       "2                          0                              0   \n",
       "3                          0                              0   \n",
       "4                          0                              0   \n",
       "\n",
       "   kitchen/stove/light  livingroom/ambience/motion  livingroom/couch/pressure  \\\n",
       "0               1024.0                         0.0                      268.0   \n",
       "1               1024.0                         0.0                      268.0   \n",
       "2               1024.0                         0.0                        0.0   \n",
       "3               1024.0                         0.0                      268.0   \n",
       "4               1024.0                         0.0                      268.0   \n",
       "\n",
       "   livingroom/tv/light      ts_day  occupied  \n",
       "0               1024.0  2020-03-01         1  \n",
       "1               1024.0  2020-03-01         1  \n",
       "2               1024.0  2020-03-01         1  \n",
       "3               1024.0  2020-03-01         1  \n",
       "4               1024.0  2020-03-01         1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivotDF.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95994841",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotDF = pivotDF.withColumn('home_or_away',(when(pivotDF[\"occupied\"] == 1, \"home\").otherwise(\"away\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ac63de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivotDF.repartition(1).write.format(\"csv\").option(\"header\", \"true\").save(\"gs://smarthome-326501/occupied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f31aed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['ts_day', 'occupied']\n",
    "df = pivotDF.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23832104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- balcon/door/contact: double (nullable = false)\n",
      " |-- bathroom/ambience/humidity: double (nullable = false)\n",
      " |-- bathroom/ambience/light: double (nullable = false)\n",
      " |-- bathroom/ambience/motion: double (nullable = false)\n",
      " |-- bathroom/ambience/temperature: double (nullable = false)\n",
      " |-- bathroom/washingmachine/current: integer (nullable = false)\n",
      " |-- bedroom/ambience/motion: double (nullable = false)\n",
      " |-- bedroom/ambience_under_the_bed/motion: double (nullable = false)\n",
      " |-- bedroom/bed/pressure: double (nullable = false)\n",
      " |-- bedroom/weightscale/pressure: double (nullable = false)\n",
      " |-- corridor/ambience/motion: double (nullable = false)\n",
      " |-- corridor/ilifeRobot/current: integer (nullable = false)\n",
      " |-- entrance/door/contact: double (nullable = true)\n",
      " |-- kitchen/ambience/motion: double (nullable = false)\n",
      " |-- kitchen/coffeemaker/current: integer (nullable = false)\n",
      " |-- kitchen/dishwasher/current: integer (nullable = false)\n",
      " |-- kitchen/fridge/contact: double (nullable = false)\n",
      " |-- kitchen/kettle/current: integer (nullable = false)\n",
      " |-- kitchen/microwave/current: integer (nullable = false)\n",
      " |-- kitchen/sandwichmaker/current: integer (nullable = false)\n",
      " |-- kitchen/stove/light: double (nullable = false)\n",
      " |-- livingroom/ambience/motion: double (nullable = false)\n",
      " |-- livingroom/couch/pressure: double (nullable = false)\n",
      " |-- livingroom/tv/light: double (nullable = false)\n",
      " |-- home_or_away: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a01cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in functions we will need\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16159d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = df.columns # Collect the column names as a list\n",
    "input_columns = input_columns[1:-1] # keep only relevant columns: from column 1 to \n",
    "\n",
    "dependent_var = 'home_or_away'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62003d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:>                                                         (0 + 8) / 9]\r"
     ]
    }
   ],
   "source": [
    "# change label (class variable) to string type to prep for reindexing\n",
    "# Pyspark is expecting a zero indexed integer for the label column. \n",
    "# Just in case our data is not in that format... we will treat it by using the StringIndexer built in method\n",
    "renamed = df.withColumn(\"label_str\", df[dependent_var].cast(StringType())) #Rename and change to string type\n",
    "indexer = StringIndexer(inputCol=\"label_str\", outputCol=\"label\") #Pyspark is expecting the this naming convention \n",
    "indexed = indexer.fit(renamed).transform(renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc359f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string type data in the input column list to numeric\n",
    "# Otherwise the Algorithm will not be able to process it\n",
    "\n",
    "# Also we will use these lists later on\n",
    "numeric_inputs = []\n",
    "string_inputs = []\n",
    "for column in input_columns:\n",
    "    # First identify the string vars in your input column list\n",
    "    if str(indexed.schema[column].dataType) == 'StringType':\n",
    "        # Set up your String Indexer function\n",
    "        indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\") \n",
    "        # Then call on the indexer you created here\n",
    "        indexed = indexer.fit(indexed).transform(indexed)\n",
    "        # Rename the column to a new name so you can disinguish it from the original\n",
    "        new_col_name = column+\"_num\"\n",
    "        # Add the new column name to the string inputs list\n",
    "        string_inputs.append(new_col_name)\n",
    "    else:\n",
    "        # If no change was needed, take no action \n",
    "        # And add the numeric var to the num list\n",
    "        numeric_inputs.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat for skewness\n",
    "# Flooring and capping\n",
    "# Plus if right skew take the log +1\n",
    "# if left skew do exp transformation\n",
    "# This is best practice\n",
    "\n",
    "# create empty dictionary d\n",
    "d = {}\n",
    "# Create a dictionary of quantiles from your numeric cols\n",
    "# I'm doing the top and bottom 1% but you can adjust if needed\n",
    "for col in numeric_inputs: \n",
    "    d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25) #if you want to make it go faster increase the last number\n",
    "\n",
    "#Now check for skewness for all numeric cols\n",
    "for col in numeric_inputs:\n",
    "    skew = indexed.agg(skewness(indexed[col])).collect() #check for skewness\n",
    "    skew = skew[0][0]\n",
    "    # If skewness is found,\n",
    "    # This function will make the appropriate corrections\n",
    "    if skew > 1: # If right skew, floor, cap and log(x+1)\n",
    "        indexed = indexed.withColumn(col, \\\n",
    "        log(when(df[col] < d[col][0],d[col][0])\\\n",
    "        .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "        .otherwise(indexed[col] ) +1).alias(col))\n",
    "        print(col+\" has been treated for positive (right) skewness. (skew =)\",skew,\")\")\n",
    "    elif skew < -1: # If left skew floor, cap and exp(x)\n",
    "        indexed = indexed.withColumn(col, \\\n",
    "        exp(when(df[col] < d[col][0],d[col][0])\\\n",
    "        .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "        .otherwise(indexed[col] )).alias(col))\n",
    "        print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a611cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check for negative values in the dataframe. \n",
    "# Produce a warning if there are negative values in the dataframe that Naive Bayes cannot be used. \n",
    "# Note: we only need to check the numeric input values since anything that is indexed won't have negative values\n",
    "\n",
    "# Calculate the mins for all columns in the df\n",
    "minimums = df.select([min(c).alias(c) for c in df.columns if c in numeric_inputs]) \n",
    "# Create an array for all mins and select only the input cols\n",
    "min_array = minimums.select(array(numeric_inputs).alias(\"mins\")) \n",
    "# Collect golobal min as Python object\n",
    "df_minimum = min_array.select(array_min(min_array.mins)).collect() \n",
    "# Slice to get the number itself\n",
    "df_minimum = df_minimum[0][0] \n",
    "\n",
    "# If there are ANY negative vals found in the df, print a warning message\n",
    "if df_minimum < 0:\n",
    "    print(\"WARNING: The Naive Bayes Classifier will not be able to process your dataframe as it contains negative values\")\n",
    "else:\n",
    "    print(\"No negative values were found in your dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we correct for negative values that may have been found above, \n",
    "# We need to vectorize our df\n",
    "# becauase the function that we use to make that correction requires a vector. \n",
    "# Now create your final features list\n",
    "features_list = numeric_inputs + string_inputs\n",
    "# Create your vector assembler object\n",
    "assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
    "# And call on the vector assembler to transform your dataframe\n",
    "output = assembler.transform(indexed).select('features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mix max scaler object \n",
    "# This is what will correct for negative values\n",
    "# I like to use a high range like 1,000 \n",
    "#     because I only see one decimal place in the final_data.show() call\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",min=0,max=1000)\n",
    "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(output)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaled_data = scalerModel.transform(output)\n",
    "final_data = scaled_data.select('label','scaledFeatures')\n",
    "# Rename to default value\n",
    "final_data = final_data.withColumnRenamed(\"scaledFeatures\",\"features\")\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ce4a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a1bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our evaluation objects\n",
    "Bin_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction') #labelCol='label'\n",
    "# Bin_evaluator = BinaryClassificationEvaluator() #labelCol='label'\n",
    "MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32165e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7acec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = fitModel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the most simplistic approach which does not use cross validation\n",
    "# Let's go ahead and train a Logistic Regression Algorithm\n",
    "classifier = LogisticRegression()\n",
    "fitModel = classifier.fit(train)\n",
    "\n",
    "# Evaluation method for binary classification problem\n",
    "predictionAndLabels = fitModel.transform(test)\n",
    "auc = Bin_evaluator.evaluate(predictionAndLabels)\n",
    "print(\"AUC:\",auc)\n",
    "\n",
    "# Evaluation for a multiclass classification problem\n",
    "predictions = fitModel.transform(test)\n",
    "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "print(\"Accuracy: {0:.2f}\".format(accuracy),\"%\") #     print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c3c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First tell Spark which classifier you want to use\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Then Set up your parameter grid for the cross validator to conduct hyperparameter tuning\n",
    "paramGrid = (ParamGridBuilder().addGrid(classifier.maxIter, [10, 15,20]).build())\n",
    "\n",
    "# Then set up the Cross Validator which requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MC_evaluator,\n",
    "                          numFolds=2) # 3 + is best practice\n",
    "\n",
    "# Then fit your model\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Collect the best model and\n",
    "# print the coefficient matrix\n",
    "# These values should be compared relative to eachother\n",
    "# And intercepts can be prepared to other models\n",
    "BestModel = fitModel.bestModel\n",
    "print(\"Intercept: \" + str(BestModel.interceptVector))\n",
    "print(\"Coefficients: \\n\" + str(BestModel.coefficientMatrix))\n",
    "\n",
    "# You can extract the best model from this run like this if you want\n",
    "LR_BestModel = BestModel\n",
    "\n",
    "# Next you need to generate predictions on the test dataset\n",
    "# fitModel automatically uses the best model \n",
    "# so we don't need to use BestModel here\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "# Now print the accuracy rate of the model or AUC for a binary classifier\n",
    "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a01ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip input_columns qith feature importance scores and create df\n",
    "\n",
    "# First convert featureimportance scores from numpy array to list\n",
    "coeff_array = BestModel.coefficientMatrix.toArray()\n",
    "coeff_scores = []\n",
    "for x in coeff_array[0]:\n",
    "    coeff_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "\n",
    "# data_schema = [StructField(\"feature\", StringType(), True),StructField(\"coeff\", DecimalType(), True)]\n",
    "# final_struc = StructType(fields=data_schema)\n",
    "# result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=final_struc)\n",
    "\n",
    "result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
    "result.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Summary\n",
    "trainingSummary = LR_BestModel.summary\n",
    "\n",
    "# General Describe\n",
    "trainingSummary.predictions.describe().show()\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\" \")\n",
    "print(\"objectiveHistory: (scaled loss + regularization) at each iteration\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# for multiclass, we can inspect metrics on a per-label basis\n",
    "print(\" \")\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\" \")\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\" \")\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\" \")\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\" \")\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "# Generate confusion matrix and print (includes accuracy)\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\" \")\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
